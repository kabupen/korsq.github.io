# 最適化計算

## 参考文献

- https://www.deeplearningbook.org/contents/numerical.html

## Introduction

最適化（Optimization）とは深層学習の領域では主に、関数 $f(x)$ を最小化するという文脈で用いられる（optimization自体は関数を$x$に対して最小化 or 最大化するという意味）。関数 $f(x)$ は cost function, loss function, error function などと呼ばれたりする。

## Gradient Descent

関数 $f$ を最小化するには、$f$ の最も変化率の大きい方向を見つければよく、それは勾配（gradient）に他ならない。
勾配が正方向に大きければ（=上り坂）最適化する方向としてはその逆に向かえばよく、負方向であれば同様に逆に向かえば良い。この性質を利用して最小化する手法が最急降下法（method of steepest descent）もしくは勾配降下法（gradient descent）と呼ばれる手法である。

勾配降下法では関数の勾配（$grad f$）の値と符号に着目する。値が大きい場合は変化率が大きい場所であるので、よりそこから離れる方向にパラメーターを更新する必要がある。
また符号もどちら方向に更新するかに重要になってくる。微分係数の符号と逆側に微小にパラメータを動かすことで、関数の最小値に近づくことができる。
$$
x^\prime = x - \epsilon \nabla f(x)
$$
どれくらいの割合更新するかを$\epsilon$というパラメータで調整し、これは学習率と呼ばれるものである。

### ちなみに

ここで関数$f(x)$に対して$\bm{u}$方向の微分（方向微分）を考える。

## Stochastic gradient descent (SGD)

確率勾配降下法（SGD）は強力な最適化手法の一種であり、勾配降下法とやっていることは何も変わらない。ただしミニバッチという考え方を使って小刻みに重みを更新することになる。

まず勾配降下法を用いたパラメータの更新について。学習に使用するデータサンプルが $m$ 個あったときに、勾配降下法ではその一つ一つに対して損失関数の勾配を計算する必要が生じる：
$$
\nabla_\theta J(\theta) = \frac{1}{m} \sum_{i=1}^m \nabla_\theta L(x_i,y_i,\theta)
$$
サンプル数 $m$ 回分の勾配計算をして、その平均値を使って更新する：
$$
\theta^\prime = \theta - \epsilon \nabla_\theta J(\theta)
$$

ただしこれだと学習サンプルが膨大になったときに、勾配計算のコストも $\mathcal{O}(m)$ に比例して膨大になってしまうという問題があった。

そこで確率勾配降下法では学習サンプルをミニバッチに分割して、そのミニバッチ単位の平均の損失関数の平均を取ってその勾配を使用する方法を採る（勾配計算のタイミングに注意）：
$$
\nabla_\theta J(\theta) = \nabla_\theta \left(\frac{1}{m^\prime} \sum_{i=1}^{m^\prime} L(x_i,y_i,\theta) \right)
$$
ミニバッチの作り方はその都度ランダムにすることで、$\sum_i^{m^\prime} L$もその都度変わる。そのため局所解に陥りにくくなり（毎回同じ関数を使って最適化しないので）、より勾配勾配降下法と比べてよりロバストな手法になっている。
